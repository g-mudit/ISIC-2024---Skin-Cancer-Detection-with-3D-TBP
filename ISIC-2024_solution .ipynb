{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":8987337,"sourceType":"datasetVersion","datasetId":5412712},{"sourceId":9062527,"sourceType":"datasetVersion","datasetId":5465260},{"sourceId":9104956,"sourceType":"datasetVersion","datasetId":5495198},{"sourceId":9105684,"sourceType":"datasetVersion","datasetId":5495648},{"sourceId":9106190,"sourceType":"datasetVersion","datasetId":5495898},{"sourceId":9119975,"sourceType":"datasetVersion","datasetId":5505203},{"sourceId":9181768,"sourceType":"datasetVersion","datasetId":5549685},{"sourceId":190080588,"sourceType":"kernelVersion"},{"sourceId":193364151,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas.api.types\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n\nimport lightgbm as lgb\nfrom scipy.stats import zscore\nfrom numpy import nanmean, nanstd\nnp.seterr(invalid='ignore')\n\nimport copy\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:37.316844Z","iopub.execute_input":"2024-08-31T23:46:37.317892Z","iopub.status.idle":"2024-08-31T23:46:37.324168Z","shell.execute_reply.started":"2024-08-31T23:46:37.317852Z","shell.execute_reply":"2024-08-31T23:46:37.323155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This notebook adds accumulated noise in an effort to boost LB score...\n\n# This is an effort to intentionally OVERFIT to the LB using this strategy:\n* See https://www.kaggle.com/code/richolson/isic-2024-shake-up-lb-overfitting-simulator\n\n# THIS WILL RESULT IN OVERFITTING AND A BAD FINAL SCORE!\n* Note to self: Manually select other notebooks for competition scoring...\n\n# Before adding \"magic noise\" - it scores .184\n* The further above .184 it scores on the LB - the worse it is likely to perform on the final test set!\n* See https://www.kaggle.com/code/richolson/isic-2024-lgbm-imagenet-v5a for the original version of this notebook","metadata":{}},{"cell_type":"markdown","source":"# Magic Noise Seeds\n* Defines array of noise seeds we use to boost LB score\n* This is just for fun - will hurt final standings... (badly)","metadata":{}},{"cell_type":"code","source":"noise_test_candidate = [24]\n\n# prior seeds that helped\nmagic_noise_seeds = [2, 3, 9, 17, 21, 22]\n\nmagic_noise_seeds = magic_noise_seeds + noise_test_candidate\n\n#we are just keeping of track of seeds that didn't help (unused)\nbad_seeds = [1, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 23]\n\n#value of 0.0000003 has been determined to provide useful noise level (lower than expected)\n#this value should be kept through this experiment\nmagic_noise_factor = 0.0000003\n\nmagic_noise_seeds","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:37.326391Z","iopub.execute_input":"2024-08-31T23:46:37.326968Z","iopub.status.idle":"2024-08-31T23:46:37.33499Z","shell.execute_reply.started":"2024-08-31T23:46:37.326935Z","shell.execute_reply":"2024-08-31T23:46:37.334085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# General Setup","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/isic-2024-challenge/train-metadata.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/isic-2024-challenge/test-metadata.csv\")\n\n#verify these are set before committing!!!\nmax_estimators = 8000    \nearly_stopping_rounds = 500  #high number assures we train through any \"lucky\" cv bounces\n\n#set to false if want to see full train results / save models.... (false if want to save on GPU)\n\nSCORING, QUICK_TEST, SAVE_TRAIN = 1, 2, 3\n\n#set to QUICK_TEST when submitting to avoid wasting GPU\nmode = QUICK_TEST\n\n#ugly duckling tabular analysis\ndo_ud = True\n\n#load OOF imagenet preds\nload_train_imagepreds_and_cv = True\n\ndo_imagenet_uglyduckling = False\nimport_ud_auged_train = False\n\n#how much gaussian noise to oof image preds\n#best LB improvement with 0.005 so far \noof_imagenet_folds_noise_std = 0.005  # Standard deviation of the Gaussian noise\n\n#assures mode = SCORING on submit\nif len(df_test) > 3:\n    mode = SCORING\n\nif mode == QUICK_TEST:\n    df_train = df_train.head(50000)  #10k too few for successful train\n    max_estimators = 500\n    do_ud = False  #True for saving out augmented train data\n    \nif mode == SAVE_TRAIN:\n    max_estimators = 500\n    do_ud = True  #True for saving out augmented train data\n\n#to keep same code base across imagenet / non-imagenet notebooks\nmain_imagenet_column_name = \"imagenet_predict\"\nimage_net_columns = [main_imagenet_column_name]\n\ndf_train","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:37.336281Z","iopub.execute_input":"2024-08-31T23:46:37.33664Z","iopub.status.idle":"2024-08-31T23:46:41.894925Z","shell.execute_reply.started":"2024-08-31T23:46:37.33661Z","shell.execute_reply":"2024-08-31T23:46:41.893894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import OOF ImageNet predictions and CV fold","metadata":{}},{"cell_type":"code","source":"if load_train_imagepreds_and_cv:\n    oof_image_net_preds = pd.read_csv(\"/kaggle/input/isic-2024-imagenet-gen-2-output/oof_predictions.csv\")\n\n    # Rename the 'oof_prediction' column to 'imagenet_predict'\n    oof_image_net_preds = oof_image_net_preds.rename(columns={'oof_prediction': main_imagenet_column_name})\n\n    # Merge with df_train\n    df_train = df_train.merge(oof_image_net_preds[['fold', 'imagenet_predict']], left_index=True, right_index=True, how='left')","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:41.896198Z","iopub.execute_input":"2024-08-31T23:46:41.896519Z","iopub.status.idle":"2024-08-31T23:46:42.368726Z","shell.execute_reply.started":"2024-08-31T23:46:41.896493Z","shell.execute_reply":"2024-08-31T23:46:42.367923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adjust pAUC of OOF imagepred folds....\n* Bring the noise!\n* Keeps LGBM from considering image predictions way-too-much (decreases feature importance)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\n\ndef comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80):\n    v_gt = abs(np.floor(np.asarray(solution.values))-1)\n    v_pred = np.array([1.0 - x for x in submission.values])\n    max_fpr = abs(1-min_tpr)\n    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n    return partial_auc\n\ndef pauc_score_func(y_true, y_pred):\n    y_true = np.asarray(y_true).flatten()\n    y_pred = np.asarray(y_pred).flatten()\n    y_true_df = pd.DataFrame(y_true, columns=['target'])\n    y_pred_df = pd.DataFrame(y_pred, columns=['prediction'])\n    return comp_score(y_true_df, y_pred_df, \"\", min_tpr=0.80)\n\ndef process_imagenet_column(df, main_imagenet_column_name, target_column_name, random_seed=42, noise_mean=0, noise_std=0.01):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n        \n    noise = np.random.normal(loc=noise_mean, scale=noise_std, size=len(df))\n    df[main_imagenet_column_name] += noise\n    \n    # Clip the adjusted values to ensure they remain between 0 and 1\n    df[main_imagenet_column_name] = df[main_imagenet_column_name].clip(0, 1)\n    \n    return df\n\nprint(\"Before adjust pauc:\", pauc_score_func(df_train[\"target\"], df_train[\"imagenet_predict\"]))\n\ntarget_column_name = 'target'\nrandom_seed = 42  # Set a fixed random seed for reproducibility\n\ndf_train = process_imagenet_column(df_train, main_imagenet_column_name, target_column_name, random_seed, noise_mean=0, noise_std=oof_imagenet_folds_noise_std)\n\n# Print the final results\nprint(\"\\nFinal results:\")\nprint(df_train[[main_imagenet_column_name, target_column_name]].head())\n\nprint(\"After adjust pauc:\", pauc_score_func(df_train[\"target\"], df_train[\"imagenet_predict\"]))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:42.371911Z","iopub.execute_input":"2024-08-31T23:46:42.37222Z","iopub.status.idle":"2024-08-31T23:46:42.697047Z","shell.execute_reply.started":"2024-08-31T23:46:42.372194Z","shell.execute_reply":"2024-08-31T23:46:42.696089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do ImageNet inference for test data","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport h5py\nimport timm\nfrom torchvision import transforms\nfrom PIL import Image\nimport io\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nclass ISICDataset(Dataset):\n    def __init__(self, hdf5_file, isic_ids, targets=None, transform=None):\n        self.hdf5_file = h5py.File(hdf5_file, 'r')\n        self.isic_ids = isic_ids\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.isic_ids)\n\n    def __getitem__(self, idx):\n        img_bytes = self.hdf5_file[self.isic_ids[idx]][()]\n        img = Image.open(io.BytesIO(img_bytes))\n        img = np.array(img)\n        \n        if self.transform:\n            transformed = self.transform(image=img)\n            img = transformed['image']\n        \n        target = self.targets[idx] if self.targets is not None else torch.tensor(-1)\n        return img, target\n\n    def __del__(self):\n        self.hdf5_file.close()\n\nbase_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\n@torch.no_grad()\ndef ensemble_predict(models, test_loader, device):\n    all_predictions = []\n    \n    for inputs, _ in tqdm(test_loader, desc=\"Predicting\"):\n        inputs = inputs.to(device)\n        fold_predictions = torch.stack([model(inputs).softmax(dim=1)[:, 1] for model in models])\n        avg_predictions = fold_predictions.mean(dim=0)\n        all_predictions.append(avg_predictions.cpu())\n    \n    return torch.cat(all_predictions).numpy()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndf_test = pd.read_csv(\"/kaggle/input/isic-2024-challenge/test-metadata.csv\")\nTEST_HDF5_FILE_PATH = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\nTRAIN_HDF5_FILE_PATH = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\n\n#only for verifiying can work with larger test dataset\n#df_test = pd.read_csv(\"/kaggle/input/isic-2024-challenge/train-metadata.csv\")\n#TEST_HDF5_FILE_PATH = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\n\nmodel_configs = [\n    (\"/kaggle/input/isic-2024-multifold-v2-offsite-train/v2_model_fold_2_epoch_1.pth\", 'tf_efficientnetv2_b1'),\n    (\"/kaggle/input/isic-2024-multifold-v2-offsite-train/v2_model_fold_4_epoch_1.pth\", 'tf_efficientnetv2_b1'),\n    (\"/kaggle/input/imagenet-143lb-from-isic-2024-imagenet-model-a/best_model.pth\", 'tf_efficientnetv2_b1'),\n    (\"/kaggle/input/isic-2024-effnetv2b0-lb-0-151/effnetv2b0_151lb_isic2024.pth\", 'tf_efficientnet_b0')\n]\n\nmodels = [timm.create_model(model_type, pretrained=False, num_classes=2).to(device) for _, model_type in model_configs]\nfor model, (model_path, _) in zip(models, model_configs):\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n\ntest_dataset = ISICDataset(\n    hdf5_file=TEST_HDF5_FILE_PATH,\n    isic_ids=df_test['isic_id'].values,\n    transform=base_transform,\n)\ntest_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=4, pin_memory=True)\n\npredictions = ensemble_predict(models, test_loader, device)\ndf_test[main_imagenet_column_name] = predictions\n\nprint(df_test[main_imagenet_column_name].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:42.698561Z","iopub.execute_input":"2024-08-31T23:46:42.698915Z","iopub.status.idle":"2024-08-31T23:46:44.092454Z","shell.execute_reply.started":"2024-08-31T23:46:42.698887Z","shell.execute_reply":"2024-08-31T23:46:44.091295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ugly Duckling Imagenet Feature Extraction","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\n\nBATCH_SIZE = 128\nNUM_WORKERS = 4\n\n# Set a fixed number of threads for NumPy\nos.environ['OMP_NUM_THREADS'] = '1'\n\ndef get_infer_model(num_classes=2, model_class = \"\", model_file = \"\", is_imagenet = False):\n    \n    model = timm.create_model(model_class, pretrained=False)\n    \n    # Replace the classifier if your MODEL_PATH doesn't include it\n    # If MODEL_PATH includes the classifier, you can remove these lines\n    if not is_imagenet:\n        in_features = model.classifier.in_features\n        model.classifier = torch.nn.Linear(in_features, num_classes)\n    \n    # Load your trained weights\n    if torch.cuda.is_available(): \n        model.load_state_dict(torch.load(model_file))\n    else:\n        model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n        \n    model = model.to(device)\n    \n    return model\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n# Set seed for reproducibility\nset_seed()\n\n# Ensure deterministic GPU operations\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\ndef extract_features(model, loader, device):\n    features = []\n    isic_ids = []\n    model.eval()\n    with torch.no_grad():\n        for inputs, batch_isic_ids in tqdm(loader, desc=\"Extracting features\"):\n            inputs = inputs.to(device)\n            output = model(inputs)\n            features.append(output.cpu())\n            isic_ids.extend(batch_isic_ids)\n    return torch.cat(features).numpy(), isic_ids\n\ndef calculate_outlier_scores(features):\n    print(\"Normalizing features...\")\n    scaler = StandardScaler()\n    features_normalized = scaler.fit_transform(features)\n    \n    print(\"Calculating Isolation Forest scores...\")\n    iso_forest = IsolationForest(contamination=0.1, random_state=42, n_jobs=1)  # n_jobs=1 for determinism\n    outlier_scores_if = iso_forest.fit_predict(features_normalized)\n    outlier_scores_if = (outlier_scores_if * -1 + 1) / 2\n    \n    print(\"Calculating Local Outlier Factor scores...\")\n    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, n_jobs=1)  # n_jobs=1 for determinism\n    outlier_scores_lof = lof.fit_predict(features_normalized)\n    outlier_scores_lof = (outlier_scores_lof * -1 + 1) / 2\n    \n    print(\"Combining scores...\")\n    outlier_scores_combined = (outlier_scores_if + outlier_scores_lof) / 2\n    \n    return outlier_scores_if, outlier_scores_lof, outlier_scores_combined\n\ndef add_outlier_scores_to_df(df, isic_ids, outlier_scores_if, outlier_scores_lof, outlier_scores_combined):\n    print(\"Adding outlier scores to dataframe...\")\n    temp_df = pd.DataFrame({\n        'isic_id': isic_ids,\n        'outlier_score_if': outlier_scores_if,\n        'outlier_score_lof': outlier_scores_lof,\n        'outlier_score_combined': outlier_scores_combined\n    })\n    return df.merge(temp_df, on='isic_id', how='left')\n\ndef gpu_correlation(tensor):\n    centered = tensor - tensor.mean(dim=0)\n    cov = torch.matmul(centered.t(), centered) / (tensor.size(0) - 1)\n    std = torch.sqrt(torch.diag(cov))\n    cor = cov / torch.ger(std, std)\n    return cor\n\ndef plot_correlation_heatmap(df, target_column, feature_columns, device):\n    print(\"Plotting correlation heatmap...\")\n    tensor = torch.tensor(df[feature_columns + [target_column]].values, dtype=torch.float32).to(device)\n    corr = gpu_correlation(tensor)\n    corr_cpu = corr.cpu().numpy()\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_cpu, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n                xticklabels=feature_columns + [target_column],\n                yticklabels=feature_columns + [target_column])\n    plt.title('Correlation Heatmap: Outlier Scores vs Target')\n    plt.show()\n    \n    return corr_cpu\n\nif do_imagenet_uglyduckling:\n    print(f\"Using device: {device}\")\n\n    print(\"Loading the model...\")\n    model = get_infer_model(num_classes=2, model_class=\"tf_efficientnetv2_b1\", model_file=\"/kaggle/input/isic-2024-tf-efficientnetv2-b1/best_model.pth\", is_imagenet=False)\n\n    # Recreate DataLoaders with deterministic settings\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, \n                              pin_memory=True, worker_init_fn=seed_worker, generator=g)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, \n                             pin_memory=True, worker_init_fn=seed_worker, generator=g)\n\n    print(\"Extracting features for train set...\")\n    train_features, train_isic_ids = extract_features(model, train_loader, device)\n    print(\"Extracting features for test set...\")\n    test_features, test_isic_ids = extract_features(model, test_loader, device)\n\n    print(\"Calculating outlier scores for train set...\")\n    train_outlier_scores_if, train_outlier_scores_lof, train_outlier_scores_combined = calculate_outlier_scores(train_features)\n    print(\"Calculating outlier scores for test set...\")\n    test_outlier_scores_if, test_outlier_scores_lof, test_outlier_scores_combined = calculate_outlier_scores(test_features)\n\n    print(\"Adding outlier scores to dataframes...\")\n    df_train = add_outlier_scores_to_df(df_train, train_isic_ids, train_outlier_scores_if, train_outlier_scores_lof, train_outlier_scores_combined)\n    df_test = add_outlier_scores_to_df(df_test, test_isic_ids, test_outlier_scores_if, test_outlier_scores_lof, test_outlier_scores_combined)\n\n    outlier_score_columns = ['outlier_score_if', 'outlier_score_lof', 'outlier_score_combined']\n    corr_matrix = plot_correlation_heatmap(df_train, 'target', outlier_score_columns, device)\n\n    print(\"Correlations with target:\")\n    target_correlations = corr_matrix[-1, :-1]\n    for col, corr in zip(outlier_score_columns, target_correlations):\n        print(f\"{col}: {corr:.4f}\")\n        \n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    # Delete large variables\n    del train_features, test_features\n    del train_outlier_scores_if, train_outlier_scores_lof, train_outlier_scores_combined\n    del test_outlier_scores_if, test_outlier_scores_lof, test_outlier_scores_combined\n    del corr_matrix\n\n    # Force garbage collection\n    import gc\n    gc.collect()\n\n    image_net_columns.extend(outlier_score_columns) \n    print(image_net_columns)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:44.09439Z","iopub.execute_input":"2024-08-31T23:46:44.095122Z","iopub.status.idle":"2024-08-31T23:46:44.127806Z","shell.execute_reply.started":"2024-08-31T23:46:44.095073Z","shell.execute_reply":"2024-08-31T23:46:44.12682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial new features\n* Credit to https://www.kaggle.com/code/snnclsr/lgbm-baseline-with-new-features","metadata":{}},{"cell_type":"code","source":"num_cols = [\n    'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', \n    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n    'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', \n    'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n    'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n    'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n    'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n    'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n    'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z',\n]\n\nnum_cols = num_cols + image_net_columns\n\n# anatom_site_general (why isn't this in here?)\ncat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"]\n\ndef feature_engineering(df):\n    df[\"lesion_size_ratio\"] = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n    df[\"lesion_shape_index\"] = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n    df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n    df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n    df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n    df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n    df[\"color_uniformity\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n    df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2) \n    df[\"perimeter_to_area_ratio\"] = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n    df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n    df[\"combined_anatomical_site\"] = df[\"anatom_site_general\"] + \"_\" + df[\"tbp_lv_location\"]\n    df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n    df[\"color_consistency\"] = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n    \n    df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n    df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n    df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n    df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n    df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n    df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n    df[\"normalized_lesion_size\"] = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n    df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n    df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n    df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n    df[\"3d_lesion_orientation\"] = np.arctan2(df_train[\"tbp_lv_y\"], df_train[\"tbp_lv_x\"])\n    df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n    df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n    df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n\n    new_num_cols = [\n        \"lesion_size_ratio\", \"lesion_shape_index\", \"hue_contrast\",\n        \"luminance_contrast\", \"lesion_color_difference\", \"border_complexity\",\n        \"color_uniformity\", \"3d_position_distance\", \"perimeter_to_area_ratio\",\n        \"lesion_visibility_score\", \"symmetry_border_consistency\", \"color_consistency\",\n\n        \"size_age_interaction\", \"hue_color_std_interaction\", \"lesion_severity_index\", \n        \"shape_complexity_index\", \"color_contrast_index\", \"log_lesion_area\",\n        \"normalized_lesion_size\", \"mean_hue_difference\", \"std_dev_contrast\",\n        \"color_shape_composite_index\", \"3d_lesion_orientation\", \"overall_color_difference\",\n        \"symmetry_perimeter_interaction\", \"comprehensive_lesion_index\",\n    ]\n    new_cat_cols = [\"combined_anatomical_site\"]\n    \n    return df, new_num_cols, new_cat_cols\n\n# Generate features for test and train\ndf_train, new_num_cols, new_cat_cols = feature_engineering(df_train.copy())\ndf_test, _, _ = feature_engineering(df_test.copy())\n\nnum_cols = num_cols + new_num_cols\ncat_cols = cat_cols + new_cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:44.129963Z","iopub.execute_input":"2024-08-31T23:46:44.13024Z","iopub.status.idle":"2024-08-31T23:46:44.249276Z","shell.execute_reply.started":"2024-08-31T23:46:44.130216Z","shell.execute_reply":"2024-08-31T23:46:44.248512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional new features...","metadata":{}},{"cell_type":"code","source":"def additional_feature_engineering(df):\n    # Asymmetry features\n    df['asymmetry_ratio'] = df['tbp_lv_symm_2axis'] / df['tbp_lv_perimeterMM']\n    df['asymmetry_area_ratio'] = df['tbp_lv_symm_2axis'] / df['tbp_lv_areaMM2']\n\n    # Color variation features\n    df['color_variation_intensity'] = df['tbp_lv_norm_color'] * df['tbp_lv_deltaLBnorm']\n    df['color_contrast_ratio'] = df['tbp_lv_deltaLBnorm'] / (df['tbp_lv_L'] + 1e-5)\n\n    # Shape complexity features\n    df['shape_irregularity'] = df['tbp_lv_perimeterMM'] / (2 * np.sqrt(np.pi * df['tbp_lv_areaMM2']))\n    df['border_density'] = df['tbp_lv_norm_border'] / df['tbp_lv_perimeterMM']\n\n    # Size-related features\n    df['size_age_ratio'] = df['clin_size_long_diam_mm'] / (df['age_approx'] + 1e-5)\n    df['area_diameter_ratio'] = df['tbp_lv_areaMM2'] / (df['clin_size_long_diam_mm']**2 + 1e-5)\n\n    # Location-based features\n    df['location_size_interaction'] = df.apply(lambda row: f\"{row['tbp_lv_location_simple']}_{bin_size(row['clin_size_long_diam_mm'])}\", axis=1)\n    df['location_age_interaction'] = df.apply(lambda row: f\"{row['tbp_lv_location_simple']}_{bin_age(row['age_approx'])}\", axis=1)\n\n    # 3D position features\n    df['3d_position_norm'] = np.sqrt(df['tbp_lv_x']**2 + df['tbp_lv_y']**2 + df['tbp_lv_z']**2)\n    df['3d_position_angle_xy'] = np.arctan2(df['tbp_lv_y'], df['tbp_lv_x'])\n    df['3d_position_angle_xz'] = np.arctan2(df['tbp_lv_z'], df['tbp_lv_x'])\n\n    # Color space transformations\n    df['lab_chroma'] = np.sqrt(df['tbp_lv_A']**2 + df['tbp_lv_B']**2)\n    df['lab_hue'] = np.arctan2(df['tbp_lv_B'], df['tbp_lv_A'])\n\n    # Texture-related features\n    df['texture_contrast'] = df['tbp_lv_stdL'] / (df['tbp_lv_L'] + 1e-5)\n    df['texture_uniformity'] = 1 / (1 + df['tbp_lv_color_std_mean'])\n\n    # Color difference features\n    df['color_difference_AB'] = np.sqrt(df['tbp_lv_deltaA']**2 + df['tbp_lv_deltaB']**2)\n    df['color_difference_total'] = np.sqrt(df['tbp_lv_deltaA']**2 + df['tbp_lv_deltaB']**2 + df['tbp_lv_deltaL']**2)\n\n    # Anatomical site encoding\n    df['anatom_site_encoded'] = df['anatom_site_general'].map(anatom_site_encoding)\n\n    # Sex encoding\n    df['sex_encoded'] = df['sex'].map({'male': 0, 'female': 1})\n\n    return df\n\ndef bin_size(size):\n    if size < 5:\n        return 'very_small'\n    elif size < 10:\n        return 'small'\n    elif size < 20:\n        return 'medium'\n    else:\n        return 'large'\n\ndef bin_age(age):\n    if age < 30:\n        return 'young'\n    elif age < 60:\n        return 'middle_aged'\n    else:\n        return 'senior'\n\n# Encoding for anatomical sites\nanatom_site_encoding = {\n    'torso': 0,\n    'upper extremity': 1,\n    'lower extremity': 2,\n    'head/neck': 3,\n    'palms/soles': 4,\n    'oral/genital': 5\n}\n\n# Add these new features to your existing feature engineering pipeline\ndf_train = additional_feature_engineering(df_train)\ndf_test = additional_feature_engineering(df_test)\n\n# Update your feature lists\nnew_num_cols = [\n    'asymmetry_ratio', 'asymmetry_area_ratio', 'color_variation_intensity',\n    'color_contrast_ratio', 'shape_irregularity', 'border_density',\n    'size_age_ratio', 'area_diameter_ratio', '3d_position_norm',\n    '3d_position_angle_xy', '3d_position_angle_xz', 'lab_chroma', 'lab_hue',\n    'texture_contrast', 'texture_uniformity', 'color_difference_AB',\n    'color_difference_total', 'anatom_site_encoded', 'sex_encoded'\n]\nnew_cat_cols = ['location_size_interaction', 'location_age_interaction']\n\nnum_cols += new_num_cols\ncat_cols += new_cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:44.250378Z","iopub.execute_input":"2024-08-31T23:46:44.250636Z","iopub.status.idle":"2024-08-31T23:46:46.341155Z","shell.execute_reply.started":"2024-08-31T23:46:44.250614Z","shell.execute_reply":"2024-08-31T23:46:46.34017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tabular Ugly Ducklings\n* Computes \"ugly duckling\" features for patients and locations, adding z-scores, percentiles, counts, severity, and consistency metrics","metadata":{}},{"cell_type":"code","source":"def ugly_duckling_processing(df, num_cols):\n    ud_columns = num_cols.copy()\n    new_num_cols = []\n    \n    #if false - only do location-based ugly ducklings\n    include_patient_wide_ud = False  \n    \n    counter = 0\n    \n    def calc_ugly_duckling_scores(group, grouping):\n        nonlocal counter\n        counter += 1\n        if counter % 10 == 0: print(\".\", end=\"\", flush=True)\n        z_scores = group[ud_columns].apply(lambda x: zscore(x, nan_policy='omit'))\n        ud_scores = np.abs(z_scores)\n        prefix = 'ud_' if grouping == 'patient' else 'ud_loc_'\n        ud_scores.columns = [f'{prefix}{col}' for col in ud_columns]\n        return ud_scores\n\n    print(\"Analyzing ducklings\", end=\"\", flush=True)\n    ud_location_col = 'tbp_lv_location'\n    ud_scores_loc = df.groupby(['patient_id', ud_location_col])[ud_columns + ['patient_id', ud_location_col]].apply(\n        lambda x: calc_ugly_duckling_scores(x, 'location')\n    ).reset_index(level=[0, 1], drop=True)\n    \n    print(\"\\nConcat ducklings\")\n    df = pd.concat([df, ud_scores_loc], axis=1)\n    \n    if include_patient_wide_ud:\n        print(\"Analyzing ducklings (part 2)\", end=\"\", flush=True)\n        ud_scores_patient = df.groupby('patient_id')[ud_columns + ['patient_id']].apply(\n            lambda x: calc_ugly_duckling_scores(x, 'patient')\n        ).reset_index(level=0, drop=True)\n        df = pd.concat([df, ud_scores_patient], axis=1)\n        print()  # New line after progress indicator\n\n    print(\"Extending ducklings\")\n    new_num_cols.extend([f'ud_loc_{col}' for col in ud_columns])\n    if include_patient_wide_ud:\n        new_num_cols.extend([f'ud_{col}' for col in ud_columns])\n\n    print(\"Enhancing ugly duckling features\", end=\"\", flush=True)\n    \n    # 1. Percentile-based ugly duckling scores\n    def calc_percentile_ud_scores(group):\n        nonlocal counter\n        counter += 1\n        if counter % 10 == 0: print(\".\", end=\"\", flush=True)\n        percentiles = group[ud_columns].rank(pct=True)\n        return percentiles.add_prefix('ud_percentile_')\n    \n    counter = 0  # Reset counter for percentile calculation\n    ud_percentiles = df.groupby('patient_id')[ud_columns].apply(calc_percentile_ud_scores).reset_index(level=0, drop=True)\n    df = pd.concat([df, ud_percentiles], axis=1)\n    new_num_cols.extend([f'ud_percentile_{col}' for col in ud_columns])\n    print()  # New line after progress indicator\n\n    # 2. Ugly duckling count features\n    threshold = 2.0  # You can adjust this threshold\n    if include_patient_wide_ud:\n        ud_count = (df[[f'ud_{col}' for col in ud_columns]].abs() > threshold).sum(axis=1)\n        df['ud_count_patient'] = ud_count\n        new_num_cols.append('ud_count_patient')\n    \n    ud_count_loc = (df[[f'ud_loc_{col}' for col in ud_columns]].abs() > threshold).sum(axis=1)\n    df['ud_count_location'] = ud_count_loc\n    new_num_cols.append('ud_count_location')\n\n    # 3. Ugly duckling severity features\n    if include_patient_wide_ud:\n        df['ud_max_severity_patient'] = df[[f'ud_{col}' for col in ud_columns]].abs().max(axis=1)\n        new_num_cols.append('ud_max_severity_patient')\n    df['ud_max_severity_location'] = df[[f'ud_loc_{col}' for col in ud_columns]].abs().max(axis=1)\n    new_num_cols.append('ud_max_severity_location')\n\n    # 4. Ugly duckling consistency features\n    if include_patient_wide_ud:\n        df['ud_consistency_patient'] = df[[f'ud_{col}' for col in ud_columns]].abs().std(axis=1)\n        new_num_cols.append('ud_consistency_patient')\n    df['ud_consistency_location'] = df[[f'ud_loc_{col}' for col in ud_columns]].abs().std(axis=1)\n    new_num_cols.append('ud_consistency_location')\n\n    return df, new_num_cols\n\nif do_ud:\n    df_train, ud_num_cols = ugly_duckling_processing(df_train.copy(), num_cols)\n    df_test, _ = ugly_duckling_processing(df_test.copy(), num_cols)\n\n    # Update the list of columns to train on\n    num_cols = num_cols + ud_num_cols","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.342436Z","iopub.execute_input":"2024-08-31T23:46:46.34274Z","iopub.status.idle":"2024-08-31T23:46:46.363629Z","shell.execute_reply.started":"2024-08-31T23:46:46.342714Z","shell.execute_reply":"2024-08-31T23:46:46.362768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Assemble final list of columns to train on...","metadata":{}},{"cell_type":"code","source":"train_cols = num_cols + cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.364738Z","iopub.execute_input":"2024-08-31T23:46:46.365023Z","iopub.status.idle":"2024-08-31T23:46:46.375916Z","shell.execute_reply.started":"2024-08-31T23:46:46.364992Z","shell.execute_reply":"2024-08-31T23:46:46.375078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving train dataset to file\n* Includes imagenet(s), ugly ducklings\n* Not category encoded","metadata":{}},{"cell_type":"code","source":"if mode == SAVE_TRAIN:\n    df_train.to_csv(\"isic_2024_train_auged.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.376866Z","iopub.execute_input":"2024-08-31T23:46:46.377145Z","iopub.status.idle":"2024-08-31T23:46:46.386185Z","shell.execute_reply.started":"2024-08-31T23:46:46.377107Z","shell.execute_reply":"2024-08-31T23:46:46.385316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overrides train data with previously augmented\n* Just loads pre-calculated for quicker testing","metadata":{}},{"cell_type":"code","source":"if import_ud_auged_train:\n    #everything including catcols\n    train_cols = ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'lesion_size_ratio', 'lesion_shape_index', 'hue_contrast', 'luminance_contrast', 'lesion_color_difference', 'border_complexity', 'color_uniformity', '3d_position_distance', 'perimeter_to_area_ratio', 'lesion_visibility_score', 'symmetry_border_consistency', 'color_consistency', 'size_age_interaction', 'hue_color_std_interaction', 'lesion_severity_index', 'shape_complexity_index', 'color_contrast_index', 'log_lesion_area', 'normalized_lesion_size', 'mean_hue_difference', 'std_dev_contrast', 'color_shape_composite_index', '3d_lesion_orientation', 'overall_color_difference', 'symmetry_perimeter_interaction', 'comprehensive_lesion_index', 'asymmetry_ratio', 'asymmetry_area_ratio', 'color_variation_intensity', 'color_contrast_ratio', 'shape_irregularity', 'border_density', 'size_age_ratio', 'area_diameter_ratio', '3d_position_norm', '3d_position_angle_xy', '3d_position_angle_xz', 'lab_chroma', 'lab_hue', 'texture_contrast', 'texture_uniformity', 'color_difference_AB', 'color_difference_total', 'anatom_site_encoded', 'sex_encoded', 'ud_loc_age_approx', 'ud_loc_clin_size_long_diam_mm', 'ud_loc_tbp_lv_A', 'ud_loc_tbp_lv_Aext', 'ud_loc_tbp_lv_B', 'ud_loc_tbp_lv_Bext', 'ud_loc_tbp_lv_C', 'ud_loc_tbp_lv_Cext', 'ud_loc_tbp_lv_H', 'ud_loc_tbp_lv_Hext', 'ud_loc_tbp_lv_L', 'ud_loc_tbp_lv_Lext', 'ud_loc_tbp_lv_areaMM2', 'ud_loc_tbp_lv_area_perim_ratio', 'ud_loc_tbp_lv_color_std_mean', 'ud_loc_tbp_lv_deltaA', 'ud_loc_tbp_lv_deltaB', 'ud_loc_tbp_lv_deltaL', 'ud_loc_tbp_lv_deltaLB', 'ud_loc_tbp_lv_deltaLBnorm', 'ud_loc_tbp_lv_eccentricity', 'ud_loc_tbp_lv_minorAxisMM', 'ud_loc_tbp_lv_nevi_confidence', 'ud_loc_tbp_lv_norm_border', 'ud_loc_tbp_lv_norm_color', 'ud_loc_tbp_lv_perimeterMM', 'ud_loc_tbp_lv_radial_color_std_max', 'ud_loc_tbp_lv_stdL', 'ud_loc_tbp_lv_stdLExt', 'ud_loc_tbp_lv_symm_2axis', 'ud_loc_tbp_lv_symm_2axis_angle', 'ud_loc_tbp_lv_x', 'ud_loc_tbp_lv_y', 'ud_loc_tbp_lv_z', 'ud_loc_lesion_size_ratio', 'ud_loc_lesion_shape_index', 'ud_loc_hue_contrast', 'ud_loc_luminance_contrast', 'ud_loc_lesion_color_difference', 'ud_loc_border_complexity', 'ud_loc_color_uniformity', 'ud_loc_3d_position_distance', 'ud_loc_perimeter_to_area_ratio', 'ud_loc_lesion_visibility_score', 'ud_loc_symmetry_border_consistency', 'ud_loc_color_consistency', 'ud_loc_size_age_interaction', 'ud_loc_hue_color_std_interaction', 'ud_loc_lesion_severity_index', 'ud_loc_shape_complexity_index', 'ud_loc_color_contrast_index', 'ud_loc_log_lesion_area', 'ud_loc_normalized_lesion_size', 'ud_loc_mean_hue_difference', 'ud_loc_std_dev_contrast', 'ud_loc_color_shape_composite_index', 'ud_loc_3d_lesion_orientation', 'ud_loc_overall_color_difference', 'ud_loc_symmetry_perimeter_interaction', 'ud_loc_comprehensive_lesion_index', 'ud_loc_asymmetry_ratio', 'ud_loc_asymmetry_area_ratio', 'ud_loc_color_variation_intensity', 'ud_loc_color_contrast_ratio', 'ud_loc_shape_irregularity', 'ud_loc_border_density', 'ud_loc_size_age_ratio', 'ud_loc_area_diameter_ratio', 'ud_loc_3d_position_norm', 'ud_loc_3d_position_angle_xy', 'ud_loc_3d_position_angle_xz', 'ud_loc_lab_chroma', 'ud_loc_lab_hue', 'ud_loc_texture_contrast', 'ud_loc_texture_uniformity', 'ud_loc_color_difference_AB', 'ud_loc_color_difference_total', 'ud_loc_anatom_site_encoded', 'ud_loc_sex_encoded', 'ud_percentile_age_approx', 'ud_percentile_clin_size_long_diam_mm', 'ud_percentile_tbp_lv_A', 'ud_percentile_tbp_lv_Aext', 'ud_percentile_tbp_lv_B', 'ud_percentile_tbp_lv_Bext', 'ud_percentile_tbp_lv_C', 'ud_percentile_tbp_lv_Cext', 'ud_percentile_tbp_lv_H', 'ud_percentile_tbp_lv_Hext', 'ud_percentile_tbp_lv_L', 'ud_percentile_tbp_lv_Lext', 'ud_percentile_tbp_lv_areaMM2', 'ud_percentile_tbp_lv_area_perim_ratio', 'ud_percentile_tbp_lv_color_std_mean', 'ud_percentile_tbp_lv_deltaA', 'ud_percentile_tbp_lv_deltaB', 'ud_percentile_tbp_lv_deltaL', 'ud_percentile_tbp_lv_deltaLB', 'ud_percentile_tbp_lv_deltaLBnorm', 'ud_percentile_tbp_lv_eccentricity', 'ud_percentile_tbp_lv_minorAxisMM', 'ud_percentile_tbp_lv_nevi_confidence', 'ud_percentile_tbp_lv_norm_border', 'ud_percentile_tbp_lv_norm_color', 'ud_percentile_tbp_lv_perimeterMM', 'ud_percentile_tbp_lv_radial_color_std_max', 'ud_percentile_tbp_lv_stdL', 'ud_percentile_tbp_lv_stdLExt', 'ud_percentile_tbp_lv_symm_2axis', 'ud_percentile_tbp_lv_symm_2axis_angle', 'ud_percentile_tbp_lv_x', 'ud_percentile_tbp_lv_y', 'ud_percentile_tbp_lv_z', 'ud_percentile_lesion_size_ratio', 'ud_percentile_lesion_shape_index', 'ud_percentile_hue_contrast', 'ud_percentile_luminance_contrast', 'ud_percentile_lesion_color_difference', 'ud_percentile_border_complexity', 'ud_percentile_color_uniformity', 'ud_percentile_3d_position_distance', 'ud_percentile_perimeter_to_area_ratio', 'ud_percentile_lesion_visibility_score', 'ud_percentile_symmetry_border_consistency', 'ud_percentile_color_consistency', 'ud_percentile_size_age_interaction', 'ud_percentile_hue_color_std_interaction', 'ud_percentile_lesion_severity_index', 'ud_percentile_shape_complexity_index', 'ud_percentile_color_contrast_index', 'ud_percentile_log_lesion_area', 'ud_percentile_normalized_lesion_size', 'ud_percentile_mean_hue_difference', 'ud_percentile_std_dev_contrast', 'ud_percentile_color_shape_composite_index', 'ud_percentile_3d_lesion_orientation', 'ud_percentile_overall_color_difference', 'ud_percentile_symmetry_perimeter_interaction', 'ud_percentile_comprehensive_lesion_index', 'ud_percentile_asymmetry_ratio', 'ud_percentile_asymmetry_area_ratio', 'ud_percentile_color_variation_intensity', 'ud_percentile_color_contrast_ratio', 'ud_percentile_shape_irregularity', 'ud_percentile_border_density', 'ud_percentile_size_age_ratio', 'ud_percentile_area_diameter_ratio', 'ud_percentile_3d_position_norm', 'ud_percentile_3d_position_angle_xy', 'ud_percentile_3d_position_angle_xz', 'ud_percentile_lab_chroma', 'ud_percentile_lab_hue', 'ud_percentile_texture_contrast', 'ud_percentile_texture_uniformity', 'ud_percentile_color_difference_AB', 'ud_percentile_color_difference_total', 'ud_percentile_anatom_site_encoded', 'ud_percentile_sex_encoded', 'ud_count_location', 'ud_max_severity_location', 'ud_consistency_location', 'sex', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'combined_anatomical_site', 'location_size_interaction', 'location_age_interaction']\n    cat_cols = ['sex', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'combined_anatomical_site', 'location_size_interaction', 'location_age_interaction']\n\n    df_train = pd.read_csv(\"/kaggle/input/isic-2024-tabular-feature-generation/isic_2024_train_auged.csv\")\n\n    missing_cols = set(df_train.columns) - set(df_test.columns)\n    new_cols = pd.DataFrame({col: df_train[col].iloc[0] for col in missing_cols}, index=df_test.index).astype(df_train[list(missing_cols)].dtypes)\n    df_test = pd.concat([df_test, new_cols], axis=1)[df_train.columns]","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.387148Z","iopub.execute_input":"2024-08-31T23:46:46.38747Z","iopub.status.idle":"2024-08-31T23:46:46.4075Z","shell.execute_reply.started":"2024-08-31T23:46:46.387447Z","shell.execute_reply":"2024-08-31T23:46:46.406503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode Columns\n* This may have a bug... (but fixing didn't effect score)","metadata":{}},{"cell_type":"code","source":"# Encode categories\ncategory_encoder = OrdinalEncoder(\n    categories='auto',\n    dtype=int,\n    handle_unknown='use_encoded_value',\n    unknown_value=-2,\n    encoded_missing_value=-1,\n)\n\nX_cat = category_encoder.fit_transform(df_train[cat_cols])\nfor c, cat_col in enumerate(cat_cols):\n    df_train[cat_col] = X_cat[:, c]\n\nX_cat = category_encoder.fit_transform(df_test[cat_cols])\nfor c, cat_col in enumerate(cat_cols):\n    df_test[cat_col] = X_cat[:, c]","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.412135Z","iopub.execute_input":"2024-08-31T23:46:46.412489Z","iopub.status.idle":"2024-08-31T23:46:46.562577Z","shell.execute_reply.started":"2024-08-31T23:46:46.412457Z","shell.execute_reply":"2024-08-31T23:46:46.561713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reduce Features\n* Removing features that experiments indicated hurt score","metadata":{}},{"cell_type":"code","source":"#stuff to remove\nbase_removals_1 = [\n    \"ud_percentile_log_lesion_area\", \"ud_count_location\", \"texture_uniformity\", \"ud_loc_tbp_lv_y\",\n    \"log_lesion_area\", \"ud_percentile_symmetry_border_consistency\", \"ud_percentile_3d_position_distance\", \"ud_loc_color_contrast_ratio\",\n    \"ud_loc_3d_position_angle_xy\", \"tbp_lv_perimeterMM\", \"normalized_lesion_size\", \"ud_loc_area_diameter_ratio\",\n    \"ud_consistency_location\", \"tbp_lv_deltaA\", \"tbp_lv_C\", \"ud_percentile_tbp_lv_norm_color\",\n    \"ud_loc_std_dev_contrast\", \"ud_loc_texture_uniformity\", \"ud_loc_tbp_lv_radial_color_std_max\", \"perimeter_to_area_ratio\",\n    \"lab_hue\", \"ud_loc_color_consistency\", \"tbp_lv_B\", \"ud_percentile_age_approx\",\n    \"ud_loc_3d_position_distance\", \"ud_percentile_lesion_shape_index\", \"ud_loc_tbp_lv_area_perim_ratio\", \"tbp_lv_minorAxisMM\",\n    \"ud_loc_tbp_lv_deltaB\", \"ud_percentile_color_contrast_index\", \"ud_loc_overall_color_difference\", \"shape_complexity_index\",\n    \"ud_loc_border_complexity\", \"sex_encoded\", \"mean_hue_difference\", \"ud_loc_lesion_visibility_score\",\n    \"tbp_lv_A\", \"tbp_lv_Bext\", \"ud_loc_tbp_lv_stdL\", \"ud_loc_asymmetry_ratio\",\n    \"ud_loc_tbp_lv_deltaA\"\n    ]\n\n\n# Remove items from train_cols that are in features\ntrain_cols = [col for col in train_cols if col not in base_removals_1]\n\n\nbase_removals_2 = [\"ud_loc_tbp_lv_eccentricity\", \"ud_percentile_size_age_interaction\", \"ud_percentile_tbp_lv_C\", \"ud_percentile_lesion_visibility_score\", \"ud_percentile_tbp_lv_L\",\n                   \"ud_loc_tbp_lv_symm_2axis_angle\", \"asymmetry_area_ratio\", \"ud_loc_mean_hue_difference\", \"tbp_lv_H\", \"ud_percentile_tbp_lv_Lext\", \"ud_percentile_tbp_lv_deltaL\",\n                   \"ud_percentile_size_age_ratio\", \"comprehensive_lesion_index\", \"tbp_lv_deltaB\", \"ud_loc_symmetry_border_consistency\", \"ud_percentile_tbp_lv_perimeterMM\", \"hue_color_std_interaction\"]\n\n# Remove items from train_cols that are in features\ntrain_cols = [col for col in train_cols if col not in base_removals_2]\n\n\nbase_removals_3 = [\"tbp_lv_Hext\", \"ud_percentile_color_difference_AB\", \"hue_contrast\", \"ud_loc_color_difference_total\"]\n\n# Remove items from train_cols that are in features\ntrain_cols = [col for col in train_cols if col not in base_removals_3]\n\nprint(\"Updated train_cols:\", train_cols)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.563704Z","iopub.execute_input":"2024-08-31T23:46:46.564011Z","iopub.status.idle":"2024-08-31T23:46:46.57384Z","shell.execute_reply.started":"2024-08-31T23:46:46.563984Z","shell.execute_reply":"2024-08-31T23:46:46.572857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using folds already defined in imagenet data","metadata":{}},{"cell_type":"code","source":"if not load_train_imagepreds_and_cv:\n    gkf = GroupKFold(n_splits=5)\n\n    df_train[\"fold\"] = -1\n    for idx, (train_idx, val_idx) in enumerate(gkf.split(df_train, df_train[\"target\"], groups=df_train[\"patient_id\"])):\n        df_train.loc[val_idx, \"fold\"] = idx","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.57505Z","iopub.execute_input":"2024-08-31T23:46:46.575489Z","iopub.status.idle":"2024-08-31T23:46:46.587081Z","shell.execute_reply.started":"2024-08-31T23:46:46.575456Z","shell.execute_reply":"2024-08-31T23:46:46.586295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Assign robustness scores used as sample weights\n* Seems to help a little...","metadata":{}},{"cell_type":"code","source":"df_train['robustness_score'] = 100\n\ndf_train.loc[df_train['lesion_id'].notna(), 'robustness_score'] = 200\n\ndf_train.loc[df_train['iddx_1'] == \"Indeterminate\", 'robustness_score'] = 50\n\ndf_train['robustness_score'] += 0.25 * df_train['tbp_lv_dnn_lesion_confidence'].fillna(0)\n\ndef create_sample_weights(df, robustness_column='robustness_score', min_weight=1, max_weight=10):\n    min_score = df[robustness_column].min()\n    max_score = df[robustness_column].max()\n    weights = min_weight + (max_weight - min_weight) * (df[robustness_column] - min_score) / (max_score - min_score)\n    return weights","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.588133Z","iopub.execute_input":"2024-08-31T23:46:46.588468Z","iopub.status.idle":"2024-08-31T23:46:46.611998Z","shell.execute_reply.started":"2024-08-31T23:46:46.588445Z","shell.execute_reply":"2024-08-31T23:46:46.611302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train with early stopping ","metadata":{}},{"cell_type":"code","source":"class EarlyStoppingException(Exception):\n    pass\n\nclass EarlyStoppingByPAUC:\n    def __init__(self, stopping_cycles, period=1, eval_set=None):\n        self.stopping_cycles = stopping_cycles\n        self.period = period\n        self.eval_set = eval_set\n        self.best_score = -np.inf\n        self.best_iteration = 0\n        self.best_model = None\n        self.counter = 0\n\n    def __call__(self, env):\n        if self.eval_set is None or env.iteration % self.period != 0:\n            return False\n\n        y_true = self.eval_set[0][1]\n        y_pred = env.model.predict(self.eval_set[0][0], num_iteration=env.iteration)\n        current_score = pauc_score_func(y_true, y_pred)\n\n        print(f\"Iteration {env.iteration}, Current pAUC: {current_score:.5f}\")\n\n        if current_score > self.best_score:\n            self.best_score = current_score\n            self.best_iteration = env.iteration\n            self.best_model_state = copy.deepcopy(env.model)\n            self.counter = 0\n        else:\n            self.counter += 1\n\n        if self.counter >= self.stopping_cycles:\n            print(f\"Early stopping at iteration {env.iteration}\")\n            print(f\"Best iteration: {self.best_iteration}\")\n            print(f\"Best pAUC: {self.best_score:.5f}\")\n            raise EarlyStoppingException()\n            return True\n\n        return False\n\n    \ndef train_lgbm_with_early_stopping(df_train, train_cols, use_early_stopping=True, early_stopping_rounds=100):\n    lgb_params = {\n        'objective': 'binary',\n        \"random_state\": 42,\n        \"n_estimators\": max_estimators,\n        'learning_rate': 0.001,\n        'num_leaves': 37,\n        'min_data_in_leaf': 57,\n        'bagging_freq': 1,\n        'pos_bagging_fraction': 0.74,\n        'neg_bagging_fraction': 0.07,\n        'feature_fraction': 0.57,\n        'lambda_l1': 0.21,\n        'lambda_l2': 0.7,\n        \"verbosity\": -1\n    }\n    scores = []\n    models = []\n    for fold in range(5):\n        df_train_fold = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n        df_valid_fold = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n        \n        train_weights = create_sample_weights(df_train_fold)\n        \n        train_dataset = lgb.Dataset(df_train_fold[train_cols], df_train_fold[\"target\"], weight=train_weights)\n        \n        valid_dataset = lgb.Dataset(df_valid_fold[train_cols], df_valid_fold[\"target\"], reference=train_dataset)\n        \n        eval_set = [(df_valid_fold[train_cols], df_valid_fold[\"target\"])]\n        callbacks = []\n        \n        if use_early_stopping:\n            rounds_between_pauc_check = 50\n            early_stopping_callback = EarlyStoppingByPAUC(\n                stopping_cycles=early_stopping_rounds // rounds_between_pauc_check,\n                period=rounds_between_pauc_check,\n                eval_set=eval_set\n            )\n            callbacks.append(early_stopping_callback)\n        \n        try:\n            model = lgb.train(\n                lgb_params,\n                train_dataset,\n                valid_sets=[valid_dataset],\n                callbacks=callbacks,\n                num_boost_round=lgb_params['n_estimators']\n            )\n\n        except EarlyStoppingException:\n            print(f\"Training stopped early.\")\n        else:\n            print(f\"Completed without early stopping\")\n\n        model = early_stopping_callback.best_model_state\n        score = early_stopping_callback.best_score\n        print(f\"Using best model from iteration {early_stopping_callback.best_iteration}\")\n        print(f\"Fold {fold} / Partial AUC Score: {score:.5f}\")            \n        \n        scores.append(score)        \n        models.append(model)\n    \n    print(\"\\nAverage pAUC:\", np.mean(scores))\n    return models, scores\n\n# Usage\nuse_early_stopping = True\nmodels, scores = train_lgbm_with_early_stopping(df_train, train_cols, use_early_stopping, early_stopping_rounds)\nnp.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:46:46.613376Z","iopub.execute_input":"2024-08-31T23:46:46.613644Z","iopub.status.idle":"2024-08-31T23:47:02.482253Z","shell.execute_reply.started":"2024-08-31T23:46:46.613621Z","shell.execute_reply":"2024-08-31T23:47:02.481304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importances\n* Interesting - but don't seem to matter much... ","metadata":{}},{"cell_type":"code","source":"importances = np.mean([model.feature_importance(importance_type='gain') for model in models], axis=0)\ndf_imp = pd.DataFrame({\n    \"feature\": models[0].feature_name(),  # Assuming all models have the same feature names\n    \"importance\": importances\n}).sort_values(\"importance\", ascending=False).reset_index(drop=True)\n\n# Print top 10 most important features\nprint(df_imp.head(30))","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:47:02.483492Z","iopub.execute_input":"2024-08-31T23:47:02.483784Z","iopub.status.idle":"2024-08-31T23:47:02.494195Z","shell.execute_reply.started":"2024-08-31T23:47:02.48376Z","shell.execute_reply":"2024-08-31T23:47:02.493257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Models","metadata":{}},{"cell_type":"code","source":"model_directory=\"saved_models\"\n\ndef save_models(models):\n    # Create the directory if it doesn't exist\n    if not os.path.exists(model_directory):\n        os.makedirs(model_directory)\n    \n    for i, model in enumerate(models):\n        model_path = os.path.join(model_directory, f\"model_fold_{i}.txt\")\n        model.save_model(model_path)\n    \n    print(f\"Models saved in {model_directory}\")\n\ndef load_models(directory=\"saved_models\"):\n    models = []\n    for i in range(5):  # Assuming 5-fold cross-validation\n        model_path = os.path.join(directory, f\"model_fold_{i}.txt\")\n        if os.path.exists(model_path):\n            model = lgb.Booster(model_file=model_path)\n            models.append(model)\n    \n    print(f\"Loaded {len(models)} models from {directory}\")\n    return models\n\nsave_models(models)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:47:02.495494Z","iopub.execute_input":"2024-08-31T23:47:02.495769Z","iopub.status.idle":"2024-08-31T23:47:02.536077Z","shell.execute_reply.started":"2024-08-31T23:47:02.495746Z","shell.execute_reply":"2024-08-31T23:47:02.535186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking Ensemble","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef get_probabilities(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)[:, 1]\n    else:\n        return sigmoid(model.predict(X))\n\ndef stacking_ensemble_lgb_new(models, X_train, y_train, X_test, train_cols, meta_model=LogisticRegression()):\n    num_models = len(models)\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros((len(y_train), num_models))\n    test_preds = np.zeros((len(X_test), num_models))\n\n    for i, model in enumerate(models):\n        # Generate out-of-fold predictions\n        for train_idx, val_idx in kf.split(X_train):\n            oof_preds[val_idx, i] = get_probabilities(model, X_train.iloc[val_idx][train_cols])\n        \n        # Generate test predictions\n        test_preds[:, i] = get_probabilities(model, X_test[train_cols])\n\n    # Train meta-model\n    meta_model.fit(oof_preds, y_train)\n    final_preds = meta_model.predict_proba(test_preds)[:, 1]\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:47:02.53725Z","iopub.execute_input":"2024-08-31T23:47:02.537607Z","iopub.status.idle":"2024-08-31T23:47:02.546843Z","shell.execute_reply.started":"2024-08-31T23:47:02.537573Z","shell.execute_reply":"2024-08-31T23:47:02.545942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Assign Predictions","metadata":{}},{"cell_type":"code","source":"#just average models (values look different than as produces by ensemble test code)\n#preds = np.mean([model.predict(df_test[train_cols]) for model in models], 0)\n\n# Stacking Ensemble\npreds = stacking_ensemble_lgb_new(models, df_train, df_train[\"target\"], df_test, train_cols)\npreds","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:47:02.548229Z","iopub.execute_input":"2024-08-31T23:47:02.549082Z","iopub.status.idle":"2024-08-31T23:47:04.870322Z","shell.execute_reply.started":"2024-08-31T23:47:02.54905Z","shell.execute_reply":"2024-08-31T23:47:04.869152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add Noise to final preds...","metadata":{}},{"cell_type":"code","source":"def generate_seeded_noise(seed, size, noise_factor):\n    np.random.seed(seed)\n    return np.random.normal(loc=0, scale=noise_factor, size=size)\n\ndef apply_stacked_noise(preds, magic_noise_seeds, noise_factor=0.025):\n    stacked_noise = np.zeros_like(preds)\n    for i, noise_seed in enumerate(magic_noise_seeds):\n        layer_noise = generate_seeded_noise(noise_seed, len(preds), noise_factor)\n        stacked_noise += layer_noise\n    \n    # Modify predictions with stacked noise\n    modified_preds = np.clip(preds + stacked_noise, 0, 1)\n    \n    return modified_preds\n\npreds = apply_stacked_noise(preds, magic_noise_seeds, magic_noise_factor)\n\npreds","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:47:04.871934Z","iopub.execute_input":"2024-08-31T23:47:04.872635Z","iopub.status.idle":"2024-08-31T23:47:04.888693Z","shell.execute_reply.started":"2024-08-31T23:47:04.872585Z","shell.execute_reply":"2024-08-31T23:47:04.88746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit!","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv(\"/kaggle/input/isic-2024-challenge/sample_submission.csv\")\ndf_sub[\"target\"] = preds\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub","metadata":{"execution":{"iopub.status.busy":"2024-08-31T23:47:04.890815Z","iopub.execute_input":"2024-08-31T23:47:04.891622Z","iopub.status.idle":"2024-08-31T23:47:04.912041Z","shell.execute_reply.started":"2024-08-31T23:47:04.891571Z","shell.execute_reply":"2024-08-31T23:47:04.911168Z"},"trusted":true},"execution_count":null,"outputs":[]}]}